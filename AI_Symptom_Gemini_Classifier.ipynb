{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio langchain-google-genai langchain tensorflow pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Image processing\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  \n",
    "\n",
    "# LLM: Gemini via Langchain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load environment variables\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize Gemini model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model loaded: sequential_1\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model (correct path based on your file)\n",
    "cnn_model = load_model(\"saved_models/dementia_cnn_sequential_1_model_V5.keras\")\n",
    "print(\"CNN model loaded:\", cnn_model.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. Class label map\n",
    "# label_map = {\n",
    "#     0: \"Non-dementia\",\n",
    "#     1: \"Very mild dementia\",\n",
    "#     2: \"Mild dementia\",\n",
    "#     3: \"Moderate dementia\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape: (1, 224, 224, 3)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# 7. Predict using the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m preds \u001b[38;5;241m=\u001b[39m cnn_model\u001b[38;5;241m.\u001b[39mpredict(img_array, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 8. Get prediction with confidence\u001b[39;00m\n\u001b[1;32m     29\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(preds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_array' is not defined"
     ]
    }
   ],
   "source": [
    "def predict_from_mri(image):\n",
    "    # 1. Convert to grayscale if not already\n",
    "    if image.mode != \"L\":\n",
    "        image = image.convert(\"L\")\n",
    "\n",
    "    # 2. Resize to match model input\n",
    "    image = image.resize((224, 224))\n",
    "\n",
    "    # 3. Convert to array\n",
    "    img_array = img_to_array(image)  # shape: (224, 224, 1)\n",
    "\n",
    "    # 4. Repeat the grayscale channel to create 3 channels (RGB format)\n",
    "    img_array = np.repeat(img_array, 3, axis=-1)  # shape: (224, 224, 3)\n",
    "\n",
    "    # 5. Normalize pixel values to [0, 1]\n",
    "    img_array = img_array / 255.0\n",
    "\n",
    "    # 6. Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # shape: (1, 224, 224, 3)\n",
    "\n",
    "    # âœ… 7. Predict using the model\n",
    "    preds = cnn_model.predict(img_array, verbose=0)\n",
    "\n",
    "    # 8. Get prediction with confidence\n",
    "    predicted_class = np.argmax(preds)\n",
    "    confidence = float(preds[0][predicted_class])\n",
    "    label = label_map[predicted_class]\n",
    "\n",
    "    # Optional: Print prediction details\n",
    "    print(f\"Model prediction: {label} ({confidence:.2f} confidence)\")\n",
    "    print(\"Class probabilities:\")\n",
    "    for i, prob in enumerate(preds[0]):\n",
    "        print(f\"  {label_map[i]}: {prob:.2f}\")\n",
    "\n",
    "    return label, confidence, preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Function: Use Gemini to generate natural language explanation\n",
    "def generate_gemini_summary(input_text):\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a dementia diagnostic assistant.\n",
    "\n",
    "Interpret the following patient data or classification and explain what it means in simple, compassionate terms for a family or clinical audience:\n",
    "\n",
    "\"{input_text}\"\n",
    "\n",
    "Do not diagnose or suggest treatment. Just explain what it may indicate about dementia stages.\n",
    "\"\"\")\n",
    "    final_prompt = prompt.format(input_text=input_text)\n",
    "    response = llm.invoke(final_prompt)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Combined chatbot function\n",
    "def multimodal_chatbot(mri_image, symptom_text):\n",
    "    responses = []\n",
    "\n",
    "    if mri_image:\n",
    "        diagnosis, confidence, all_probs = predict_from_mri(mri_image)\n",
    "        \n",
    "        # Format probabilities for all classes\n",
    "        class_probs = [f\"{label_map[i]}: {prob:.2f}\" for i, prob in enumerate(all_probs)]\n",
    "        prob_text = \", \".join(class_probs)\n",
    "        \n",
    "        responses.append(f\"**MRI Classification:** {diagnosis} (Confidence: {confidence:.2f})\")\n",
    "        responses.append(f\"**All Probabilities:** {prob_text}\")\n",
    "        \n",
    "        explanation = generate_gemini_summary(f\"MRI indicates: {diagnosis} with {confidence:.2f} confidence\")\n",
    "        responses.append(f\"**Gemini Summary for MRI:**\\n{explanation}\")\n",
    "\n",
    "    if symptom_text:\n",
    "        explanation = generate_gemini_summary(f\"Symptoms: {symptom_text}\")\n",
    "        responses.append(f\"**Gemini Summary for Symptoms:**\\n{explanation}\")\n",
    "\n",
    "    if not responses:\n",
    "        return \"Please upload an MRI image or enter symptom information.\"\n",
    "\n",
    "    return \"\\n\\n\".join(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation test\n",
    "def validate_model(test_image_path):\n",
    "    \"\"\"Test model on a single known image\"\"\"\n",
    "    test_image = Image.open(test_image_path)\n",
    "    label, confidence, probs = predict_from_mri(test_image)\n",
    "    print(f\"Predicted: {label}, Confidence: {confidence:.4f}\")\n",
    "    print(f\"All probabilities: {probs}\")\n",
    "    return label, confidence, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 19:41:20.878085: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747100480.881737  440785 meta_optimizer.cc:967] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    }
   ],
   "source": [
    "# 10. Gradio UI\n",
    "gr.Interface(\n",
    "    fn=multimodal_chatbot,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload MRI Image\"),\n",
    "        gr.Textbox(lines=4, placeholder=\"Describe symptoms here...\", label=\"Symptom Description\")\n",
    "    ],\n",
    "    outputs=\"markdown\",\n",
    "    title=\"Dementia MRI & Symptom Assistant (Powered by Gemini)\",\n",
    "    description=\"Upload an MRI or enter symptoms. The assistant will classify dementia and explain the findings using Google's Gemini 2.0 model.\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
