{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio langchain-google-genai langchain tensorflow pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 20:56:44.063153: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/envs/dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Image processing\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  \n",
    "\n",
    "# LLM: Gemini via Langchain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load environment variables\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize Gemini model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model loaded: sequential_1\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model (correct path based on your file)\n",
    "cnn_model = load_model(\"saved_models/dementia_cnn_sequential_1_model_V5.keras\")\n",
    "print(\"CNN model loaded:\", cnn_model.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. Class label map\n",
    "# label_map = {\n",
    "#     0: \"Non-dementia\",\n",
    "#     1: \"Very mild dementia\",\n",
    "#     2: \"Mild dementia\",\n",
    "#     3: \"Moderate dementia\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_mri(image):\n",
    "    # 1. Convert to grayscale if not already\n",
    "    if image.mode != \"L\":\n",
    "        image = image.convert(\"L\")\n",
    "\n",
    "    # 2. Resize to match model input\n",
    "    image = image.resize((224, 224))\n",
    "\n",
    "    # 3. Convert to array\n",
    "    img_array = img_to_array(image)  # shape: (224, 224, 1)\n",
    "\n",
    "    # 4. Repeat the grayscale channel to create 3 channels (RGB format)\n",
    "    img_array = np.repeat(img_array, 3, axis=-1)  # shape: (224, 224, 3)\n",
    "\n",
    "    # 5. Normalize pixel values to [0, 1]\n",
    "    img_array = img_array / 255.0\n",
    "\n",
    "    # 6. Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # shape: (1, 224, 224, 3)\n",
    "\n",
    "    # âœ… 7. Predict using the model\n",
    "    preds = cnn_model.predict(img_array, verbose=0)\n",
    "\n",
    "    # 8. Get prediction with confidence\n",
    "    predicted_class = np.argmax(preds)\n",
    "    confidence = float(preds[0][predicted_class])\n",
    "    label = label_map[predicted_class]\n",
    "\n",
    "    # Optional: Print prediction details\n",
    "    print(f\"Model prediction: {label} ({confidence:.2f} confidence)\")\n",
    "    print(\"Class probabilities:\")\n",
    "    for i, prob in enumerate(preds[0]):\n",
    "        print(f\"  {label_map[i]}: {prob:.2f}\")\n",
    "\n",
    "    return label, confidence, preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Function: Use Gemini to generate natural language explanation\n",
    "def generate_gemini_summary(input_text):\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a dementia diagnostic assistant.\n",
    "\n",
    "Interpret the following patient data or classification and explain what it means in simple, compassionate terms for a family or clinical audience:\n",
    "\n",
    "\"{input_text}\"\n",
    "\n",
    "Do not diagnose or suggest treatment. Just explain what it may indicate about dementia stages.\n",
    "\"\"\")\n",
    "    final_prompt = prompt.format(input_text=input_text)\n",
    "    response = llm.invoke(final_prompt)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Combined chatbot function\n",
    "def multimodal_chatbot(mri_image, symptom_text):\n",
    "    responses = []\n",
    "\n",
    "    if mri_image:\n",
    "        diagnosis, confidence, all_probs = predict_from_mri(mri_image)\n",
    "        \n",
    "        # Format probabilities for all classes\n",
    "        class_probs = [f\"{label_map[i]}: {prob:.2f}\" for i, prob in enumerate(all_probs)]\n",
    "        prob_text = \", \".join(class_probs)\n",
    "        \n",
    "        responses.append(f\"**MRI Classification:** {diagnosis} (Confidence: {confidence:.2f})\")\n",
    "        responses.append(f\"**All Probabilities:** {prob_text}\")\n",
    "        \n",
    "        explanation = generate_gemini_summary(f\"MRI indicates: {diagnosis} with {confidence:.2f} confidence\")\n",
    "        responses.append(f\"**Gemini Summary for MRI:**\\n{explanation}\")\n",
    "\n",
    "    if symptom_text:\n",
    "        explanation = generate_gemini_summary(f\"Symptoms: {symptom_text}\")\n",
    "        responses.append(f\"**Gemini Summary for Symptoms:**\\n{explanation}\")\n",
    "\n",
    "    if not responses:\n",
    "        return \"Please upload an MRI image or enter symptom information.\"\n",
    "\n",
    "    return \"\\n\\n\".join(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation test\n",
    "def validate_model(test_image_path):\n",
    "    \"\"\"Test model on a single known image\"\"\"\n",
    "    test_image = Image.open(test_image_path)\n",
    "    label, confidence, probs = predict_from_mri(test_image)\n",
    "    print(f\"Predicted: {label}, Confidence: {confidence:.4f}\")\n",
    "    print(f\"All probabilities: {probs}\")\n",
    "    return label, confidence, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: VeryMildDemented (0.32 confidence)\n",
      "Class probabilities:\n",
      "  NonDemented: 0.29\n",
      "  VeryMildDemented: 0.32\n",
      "  MildDemented: 0.23\n",
      "  ModerateDemented: 0.17\n"
     ]
    }
   ],
   "source": [
    "# 10. Gradio UI\n",
    "gr.Interface(\n",
    "    fn=multimodal_chatbot,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload MRI Image\"),\n",
    "        gr.Textbox(lines=4, placeholder=\"Describe symptoms here...\", label=\"Symptom Description\")\n",
    "    ],\n",
    "    outputs=\"markdown\",\n",
    "    title=\"Dementia MRI & Symptom Assistant (Powered by Gemini)\",\n",
    "    description=\"Upload an MRI or enter symptoms. The assistant will classify dementia and explain the findings using Google's Gemini 2.0 model.\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
