{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio langchain-google-genai langchain tensorflow pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Image processing\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  \n",
    "\n",
    "# LLM: Gemini via Langchain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load environment variables\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize Gemini model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747188999.533444 1400657 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1747188999.533470 1400657 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model loaded: sequential\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model (correct path based on your file)\n",
    "cnn_model = load_model(\"saved_models/dementia_cnn_sequential_model_V10.keras\")\n",
    "print(\"CNN model loaded:\", cnn_model.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. Class label map\n",
    "# label_map = {\n",
    "#     0: \"Non-dementia\",\n",
    "#     1: \"Very mild dementia\",\n",
    "#     2: \"Mild dementia\",\n",
    "#     3: \"Moderate dementia\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_mri(image):\n",
    "    # 1. Convert to grayscale if not already\n",
    "    if image.mode != \"L\":\n",
    "        image = image.convert(\"L\")\n",
    "\n",
    "    # 2. Resize to match model input\n",
    "    image = image.resize((224, 224))\n",
    "\n",
    "    # 3. Convert to array\n",
    "    img_array = img_to_array(image)  # shape: (224, 224, 1)\n",
    "\n",
    "    # 4. Repeat the grayscale channel to create 3 channels (RGB format)\n",
    "    img_array = np.repeat(img_array, 3, axis=-1)  # shape: (224, 224, 3)\n",
    "\n",
    "    # 5. Normalize pixel values to [0, 1]\n",
    "    img_array = img_array / 255.0\n",
    "\n",
    "    # 6. Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # shape: (1, 224, 224, 3)\n",
    "\n",
    "    # âœ… 7. Predict using the model\n",
    "    preds = cnn_model.predict(img_array, verbose=0)\n",
    "\n",
    "    # 8. Get prediction with confidence\n",
    "    predicted_class = np.argmax(preds)\n",
    "    confidence = float(preds[0][predicted_class])\n",
    "    label = label_map[predicted_class]\n",
    "\n",
    "    # Optional: Print prediction details\n",
    "    print(f\"Model prediction: {label} ({confidence:.2f} confidence)\")\n",
    "    print(\"Class probabilities:\")\n",
    "    for i, prob in enumerate(preds[0]):\n",
    "        print(f\"  {label_map[i]}: {prob:.2f}\")\n",
    "\n",
    "    return label, confidence, preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Function: Use Gemini to generate natural language explanation\n",
    "def generate_gemini_summary(input_text):\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a dementia diagnostic assistant.\n",
    "\n",
    "Interpret the following patient data or classification and explain what it means in simple, compassionate terms for a family or clinical audience:\n",
    "\n",
    "\"{input_text}\"\n",
    "\n",
    "Do not diagnose or suggest treatment. Just explain what it may indicate about dementia stages.\n",
    "\"\"\")\n",
    "    final_prompt = prompt.format(input_text=input_text)\n",
    "    response = llm.invoke(final_prompt)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Combined chatbot function\n",
    "def multimodal_chatbot(mri_image, symptom_text):\n",
    "    responses = []\n",
    "\n",
    "    if mri_image:\n",
    "        diagnosis, confidence, all_probs = predict_from_mri(mri_image)\n",
    "        \n",
    "        # Format probabilities for all classes\n",
    "        class_probs = [f\"{label_map[i]}: {prob:.2f}\" for i, prob in enumerate(all_probs)]\n",
    "        prob_text = \", \".join(class_probs)\n",
    "        \n",
    "        responses.append(f\"**MRI Classification:** {diagnosis} (Confidence: {confidence:.2f})\")\n",
    "        responses.append(f\"**All Probabilities:** {prob_text}\")\n",
    "        \n",
    "        explanation = generate_gemini_summary(f\"MRI indicates: {diagnosis} with {confidence:.2f} confidence\")\n",
    "        responses.append(f\"**Gemini Summary for MRI:**\\n{explanation}\")\n",
    "\n",
    "    if symptom_text:\n",
    "        explanation = generate_gemini_summary(f\"Symptoms: {symptom_text}\")\n",
    "        responses.append(f\"**Gemini Summary for Symptoms:**\\n{explanation}\")\n",
    "\n",
    "    if not responses:\n",
    "        return \"Please upload an MRI image or enter symptom information.\"\n",
    "\n",
    "    return \"\\n\\n\".join(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation test\n",
    "def validate_model(test_image_path):\n",
    "    \"\"\"Test model on a single known image\"\"\"\n",
    "    test_image = Image.open(test_image_path)\n",
    "    label, confidence, probs = predict_from_mri(test_image)\n",
    "    print(f\"Predicted: {label}, Confidence: {confidence:.4f}\")\n",
    "    print(f\"All probabilities: {probs}\")\n",
    "    return label, confidence, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/qn/scf2lm251mvfhk1h4mm8nlmw0000gn/T/ipykernel_2966/795729735.py\", line 6, in multimodal_chatbot\n",
      "    diagnosis, confidence, all_probs = predict_from_mri(mri_image)\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/qn/scf2lm251mvfhk1h4mm8nlmw0000gn/T/ipykernel_2966/1784069301.py\", line 22, in predict_from_mri\n",
      "    preds = cnn_model.predict(img_array, verbose=0)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/keras/src/layers/input_spec.py\", line 227, in assert_input_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: Exception encountered when calling Sequential.call().\n",
      "\n",
      "\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 100352, but received input with shape (1, 86528)\u001b[0m\n",
      "\n",
      "Arguments received by Sequential.call():\n",
      "  â€¢ inputs=tf.Tensor(shape=(1, 224, 224, 3), dtype=float32)\n",
      "  â€¢ training=False\n",
      "  â€¢ mask=None\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/qn/scf2lm251mvfhk1h4mm8nlmw0000gn/T/ipykernel_2966/795729735.py\", line 6, in multimodal_chatbot\n",
      "    diagnosis, confidence, all_probs = predict_from_mri(mri_image)\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/qn/scf2lm251mvfhk1h4mm8nlmw0000gn/T/ipykernel_2966/1784069301.py\", line 22, in predict_from_mri\n",
      "    preds = cnn_model.predict(img_array, verbose=0)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/keras/src/layers/input_spec.py\", line 227, in assert_input_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: Exception encountered when calling Sequential.call().\n",
      "\n",
      "\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 100352, but received input with shape (1, 86528)\u001b[0m\n",
      "\n",
      "Arguments received by Sequential.call():\n",
      "  â€¢ inputs=tf.Tensor(shape=(1, 224, 224, 3), dtype=float32)\n",
      "  â€¢ training=False\n",
      "  â€¢ mask=None\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/gradio/utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/qn/scf2lm251mvfhk1h4mm8nlmw0000gn/T/ipykernel_2966/795729735.py\", line 6, in multimodal_chatbot\n",
      "    diagnosis, confidence, all_probs = predict_from_mri(mri_image)\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/qn/scf2lm251mvfhk1h4mm8nlmw0000gn/T/ipykernel_2966/1784069301.py\", line 22, in predict_from_mri\n",
      "    preds = cnn_model.predict(img_array, verbose=0)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.12/site-packages/keras/src/layers/input_spec.py\", line 227, in assert_input_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: Exception encountered when calling Sequential.call().\n",
      "\n",
      "\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 100352, but received input with shape (1, 86528)\u001b[0m\n",
      "\n",
      "Arguments received by Sequential.call():\n",
      "  â€¢ inputs=tf.Tensor(shape=(1, 224, 224, 3), dtype=float32)\n",
      "  â€¢ training=False\n",
      "  â€¢ mask=None\n"
     ]
    }
   ],
   "source": [
    "# 10. Gradio UI\n",
    "gr.Interface(\n",
    "    fn=multimodal_chatbot,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload MRI Image\"),\n",
    "        gr.Textbox(lines=4, placeholder=\"Describe symptoms here...\", label=\"Symptom Description\")\n",
    "    ],\n",
    "    outputs=\"markdown\",\n",
    "    title=\"Dementia MRI & Symptom Assistant (Powered by Gemini)\",\n",
    "    description=\"Upload an MRI or enter symptoms. The assistant will classify dementia and explain the findings using Google's Gemini 2.0 model.\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
