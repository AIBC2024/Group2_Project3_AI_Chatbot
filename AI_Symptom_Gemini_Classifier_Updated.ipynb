{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio langchain-google-genai langchain tensorflow pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Image processing\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  \n",
    "\n",
    "# LLM: Gemini via Langchain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load environment variables\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize Gemini model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747362460.912600  196104 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1747362460.912625  196104 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model loaded: sequential\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dev/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 18 variables whereas the saved optimizer has 34 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model (correct path based on your file)\n",
    "cnn_model = load_model(\"saved_models/dementia_cnn_sequential_model_V13.keras\")\n",
    "print(\"CNN model loaded:\", cnn_model.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. Class label map\n",
    "# label_map = {\n",
    "#     0: \"Non-dementia\",\n",
    "#     1: \"Very mild dementia\",\n",
    "#     2: \"Mild dementia\",\n",
    "#     3: \"Moderate dementia\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_mri(image):\n",
    "    # 1. Convert to RGB if not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # 2. Resize to match model input\n",
    "    image = image.resize((128, 128))\n",
    "\n",
    "    # 3. Convert to array\n",
    "    img_array = img_to_array(image)  # shape: (224, 224, 3)\n",
    "\n",
    "    # 4. Normalize pixel values to [0, 1]\n",
    "    img_array = img_array / 255.0\n",
    "\n",
    "    # 5. Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # shape: (1, 224, 224, 3)\n",
    "\n",
    "    # âœ… 6. Predict using the model\n",
    "    preds = cnn_model.predict(img_array, verbose=0)\n",
    "\n",
    "    # 7. Get prediction with confidence\n",
    "    predicted_class = np.argmax(preds)\n",
    "    confidence = float(preds[0][predicted_class])\n",
    "    label = label_map[predicted_class]\n",
    "\n",
    "    # Optional: Print prediction details\n",
    "    print(f\"Model prediction: {label} ({confidence:.2f} confidence)\")\n",
    "    print(\"Class probabilities:\")\n",
    "    for i, prob in enumerate(preds[0]):\n",
    "        print(f\"  {label_map[i]}: {prob:.2f}\")\n",
    "\n",
    "    return label, confidence, preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Function: Use Gemini to generate natural language explanation\n",
    "def generate_gemini_summary(input_text):\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a dementia diagnostic assistant.\n",
    "\n",
    "Interpret the following patient data or classification and explain what it means in simple, compassionate terms for a family or clinical audience:\n",
    "\n",
    "\"{input_text}\"\n",
    "\n",
    "Do not diagnose or suggest treatment. Just explain what it may indicate about dementia stages.\n",
    "\"\"\")\n",
    "    final_prompt = prompt.format(input_text=input_text)\n",
    "    response = llm.invoke(final_prompt)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 9. Combined chatbot function\n",
    "def multimodal_chatbot(mri_image, symptom_text):\n",
    "    responses = []\n",
    "\n",
    "    if mri_image:\n",
    "        diagnosis, confidence, all_probs = predict_from_mri(mri_image)\n",
    "        \n",
    "        # Format probabilities for all classes\n",
    "        class_probs = [f\"{label_map[i]}: {prob:.2f}\" for i, prob in enumerate(all_probs)]\n",
    "        prob_text = \", \".join(class_probs)\n",
    "        \n",
    "        responses.append(f\"**MRI Classification:** {diagnosis} (Confidence: {confidence:.2f})\")\n",
    "        responses.append(f\"**All Probabilities:** {prob_text}\")\n",
    "        \n",
    "        explanation = generate_gemini_summary(f\"MRI indicates: {diagnosis} with {confidence:.2f} confidence\")\n",
    "        responses.append(f\"**Gemini Summary for MRI:**\\n{explanation}\")\n",
    "\n",
    "    if symptom_text:\n",
    "        explanation = generate_gemini_summary(f\"Symptoms: {symptom_text}\")\n",
    "        responses.append(f\"**Gemini Summary for Symptoms:**\\n{explanation}\")\n",
    "\n",
    "    if not responses:\n",
    "        return \"Please upload an MRI image or enter symptom information.\"\n",
    "\n",
    "    return \"\\n\\n\".join(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_chatbot(mri_image, symptom_text):\n",
    "    responses = []\n",
    "\n",
    "    mri_text = \"\"\n",
    "    symptom_summary = \"\"\n",
    "\n",
    "    if mri_image:\n",
    "        diagnosis, confidence, all_probs = predict_from_mri(mri_image)\n",
    "\n",
    "        # Format probabilities for all classes\n",
    "        class_probs = [f\"{label_map[i]}: {prob:.2f}\" for i, prob in enumerate(all_probs)]\n",
    "        prob_text = \", \".join(class_probs)\n",
    "\n",
    "        responses.append(f\"**MRI Classification:** {diagnosis} (Confidence: {confidence:.2f})\")\n",
    "        responses.append(f\"**All Probabilities:** {prob_text}\")\n",
    "\n",
    "        mri_text = f\"MRI indicates possible diagnosis: **{diagnosis}** with confidence of {confidence:.2f}. Probabilities: {prob_text}.\"\n",
    "\n",
    "    if symptom_text:\n",
    "        symptom_summary = f\"The patient also reports the following symptoms: {symptom_text.strip()}.\"\n",
    "\n",
    "    if not mri_image and not symptom_text:\n",
    "        return \"Please upload an MRI image or enter symptom information.\"\n",
    "\n",
    "    # Weighted multimodal reasoning: 50% MRI, 50% Symptoms\n",
    "    if mri_text and symptom_summary:\n",
    "        combined_input = (\n",
    "            f\"Using 50% weight from imaging and 50% from symptoms, consider the following context:\\n\\n\"\n",
    "            f\"{mri_text}\\n\\n\"\n",
    "            f\"{symptom_summary}\\n\\n\"\n",
    "            f\"Provide an overall diagnostic reasoning based primarily on the MRI, but also lightly informed by symptoms.\"\n",
    "        )\n",
    "        summary = generate_gemini_summary(combined_input)\n",
    "        responses.append(f\"**Gemini Combined Diagnostic Summary (50% MRI weighted):**\\n{summary}\")\n",
    "\n",
    "    elif mri_text:\n",
    "        explanation = generate_gemini_summary(mri_text)\n",
    "        responses.append(f\"**Gemini Summary for MRI:**\\n{explanation}\")\n",
    "\n",
    "    elif symptom_summary:\n",
    "        explanation = generate_gemini_summary(symptom_summary)\n",
    "        responses.append(f\"**Gemini Summary for Symptoms:**\\n{explanation}\")\n",
    "\n",
    "    return \"\\n\\n\".join(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation test\n",
    "def validate_model(test_image_path):\n",
    "    \"\"\"Test model on a single known image\"\"\"\n",
    "    test_image = Image.open(test_image_path)\n",
    "    label, confidence, probs = predict_from_mri(test_image)\n",
    "    print(f\"Predicted: {label}, Confidence: {confidence:.4f}\")\n",
    "    print(f\"All probabilities: {probs}\")\n",
    "    return label, confidence, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: VeryMildDemented (1.00 confidence)\n",
      "Class probabilities:\n",
      "  NonDemented: 0.00\n",
      "  VeryMildDemented: 1.00\n",
      "  MildDemented: 0.00\n",
      "  ModerateDemented: 0.00\n",
      "Model prediction: VeryMildDemented (1.00 confidence)\n",
      "Class probabilities:\n",
      "  NonDemented: 0.00\n",
      "  VeryMildDemented: 1.00\n",
      "  MildDemented: 0.00\n",
      "  ModerateDemented: 0.00\n"
     ]
    }
   ],
   "source": [
    "# 10. Gradio UI\n",
    "gr.Interface(\n",
    "    fn=multimodal_chatbot,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload MRI Image\"),\n",
    "        gr.Textbox(lines=4, placeholder=\"Describe symptoms here...\", label=\"Symptom Description\")\n",
    "    ],\n",
    "    outputs=\"markdown\",\n",
    "    title=\"Dementia MRI & Symptom Assistant (Powered by Gemini)\",\n",
    "    description=\"Upload an MRI or enter symptoms. The assistant will classify dementia and explain the findings using Google's Gemini 2.0 model.\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beb2b039",
   "metadata": {},
   "source": [
    "### âœ… Updated prediction function for grayscale MRI input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835945e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "\n",
    "def predict_from_mri(image):\n",
    "    # 1. Convert to grayscale explicitly\n",
    "    if image.mode != \"L\":\n",
    "        image = image.convert(\"L\")  # Ensure grayscale\n",
    "\n",
    "    # 2. Resize image to match model input size\n",
    "    image = image.resize((224, 224))\n",
    "\n",
    "    # 3. Convert image to array with 1 channel\n",
    "    img_array = img_to_array(image)  # shape: (224, 224, 1)\n",
    "\n",
    "    # 4. Normalize pixel values to [0, 1]\n",
    "    img_array = img_array / 255.0\n",
    "\n",
    "    # 5. Add batch dimension: (1, 224, 224, 1)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # 6. Make prediction using your trained model\n",
    "    preds = cnn_model.predict(img_array, verbose=0)\n",
    "\n",
    "    # 7. Get predicted class and confidence\n",
    "    predicted_class = np.argmax(preds)\n",
    "    confidence = float(preds[0][predicted_class])\n",
    "    label = label_map[predicted_class]\n",
    "\n",
    "    # 8. Print details\n",
    "    print(f\"Model prediction: {label} ({confidence:.2f} confidence)\")\n",
    "    for i, prob in enumerate(preds[0]):\n",
    "        print(f\"  {label_map[i]}: {prob:.2f}\")\n",
    "\n",
    "    return label, confidence, preds[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
