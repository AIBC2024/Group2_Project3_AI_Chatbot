{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot to upload dementia brain scan image located in the /data_split_trainTest/test folder.\n",
    "You can select any images under MildDemented, ModerateDemented, NonDemented, or VeryMildDemented folder\n",
    "Note: We chose 60% for the model to use the image and 40% for the text diagnostic.\n",
    "So keep in mind this is only an exploratory project and we won't be able to hit 100% accuracy\n",
    "\n",
    "At the Gradio interface you can also add patient diagnostic.\n",
    "\n",
    "Note: Image is required. Diagnostic is optional but can be added later. End result, Gemini LLM will give you a diagnostic. Remember this is only an exploratory project, not to replace a true doctor's diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: gradio in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (5.26.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (2.2.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.9.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (1.9.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.30.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (2.10.4)\n",
      "Requirement already satisfied: pydub in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.11.7)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio-client==1.9.0->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from gradio-client==1.9.0->gradio) (14.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/dev/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow gradio transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108f285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import re\n",
    "import os\n",
    "#import google.generativeai as genai\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf09d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:31:33.225910: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Pro\n",
      "2025-05-18 20:31:33.225935: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 48.00 GB\n",
      "2025-05-18 20:31:33.225941: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 18.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747621893.225959  847872 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1747621893.225984  847872 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load trained Keras model from correct path\n",
    "# Remember this Keras file is very big, go to https://www.dropbox.com/scl/fo/mmtv94e8t4u9x7vgvqwvw/AEigp2bJ9nK4hC_juE2aVkY?rlkey=5s76fv2w7303kxymyccdinx0h&dl=0\n",
    "# Request ACCESS to download the file from Matt Le (Matt@luxiumcreative.com) from thesaved_models folder, and upload the keras file to your own local\n",
    "\n",
    "model = load_model(\"saved_models/dementia_cnn_sequential_model_V2.keras\")\n",
    "\n",
    "# Define label map\n",
    "label_map = {\n",
    "    0: \"Mild Demented\",\n",
    "    1: \"Moderate Demented\",\n",
    "    2: \"Non Demented\",\n",
    "    3: \"Very Mild Demented\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained text classifier (multi-class classification using BERT)\n",
    "sentiment_model = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "def preprocess_mri_image(img_path_or_pil):\n",
    "    model_input_shape = model.input_shape[1:3]\n",
    "    img = image.load_img(img_path_or_pil, target_size=model_input_shape)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    return np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRI prediction\n",
    "def predict_dementia_stage(img):\n",
    "    processed = preprocess_mri_image(img)\n",
    "    preds = model.predict(processed)[0]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-based text classification\n",
    "def rule_based_classifier(text):\n",
    "    text = text.lower()\n",
    "    symptoms = {\n",
    "        'moderate': [\n",
    "            r'cannot recognize', r'difficulty dressing', r'cannot function alone', r'severe memory', r'wandering', r'safety concern'\n",
    "        ],\n",
    "        'mild': [\n",
    "            r'forgetting recent events', r'losing things', r'confusion with time', r'poor judgment', r'difficulty with planning'\n",
    "        ],\n",
    "        'very_mild': [\n",
    "            r'slight forgetfulness', r'misplacing keys', r'word finding difficulty', r'mild memory problems'\n",
    "        ],\n",
    "        'none': [\n",
    "            r'no memory issue', r'normal memory', r'sharp', r'alert', r'good cognition'\n",
    "        ]\n",
    "    }\n",
    "    scores = {k: 0 for k in symptoms}\n",
    "    for stage, patterns in symptoms.items():\n",
    "        for p in patterns:\n",
    "            if re.search(p, text):\n",
    "                scores[stage] += 1\n",
    "    total = sum(scores.values())\n",
    "    if total == 0:\n",
    "        return [0.25, 0.25, 0.25, 0.25]\n",
    "    return [\n",
    "        scores['mild'] / total,\n",
    "        scores['moderate'] / total,\n",
    "        scores['none'] / total,\n",
    "        scores['very_mild'] / total\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT sentiment classifier\n",
    "\n",
    "def sentiment_classifier(text):\n",
    "    result = sentiment_model(text, top_k=1)[0]\n",
    "    label = result['label'].lower()\n",
    "    if 'negative' in label:\n",
    "        return [0.1, 0.6, 0.1, 0.2]\n",
    "    elif 'positive' in label:\n",
    "        return [0.2, 0.1, 0.6, 0.1]\n",
    "    else:\n",
    "        return [0.25, 0.25, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined text classifier\n",
    "\n",
    "def classify_text_to_dementia(text):\n",
    "    rule_probs = np.array(rule_based_classifier(text))\n",
    "    sentiment_probs = np.array(sentiment_classifier(text))\n",
    "    return (0.5 * rule_probs + 0.5 * sentiment_probs).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine MRI and text prediction\n",
    "\n",
    "def combine_predictions(mri_probs, text_probs, mri_weight=0.6): # this is the weight for MRI\n",
    "    return (mri_weight * np.array(mri_probs) + (1 - mri_weight) * np.array(text_probs)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the API Key in a variable\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Set up the Google Generative AI model for our LLMs\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(\n",
    "    model=GEMINI_MODEL,\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3b411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gemini summary for dementia stage explanation\n",
    "# ✅ Define the Gemini stage explanation function\n",
    "def generate_dementia_stage_summary(diagnosis):\n",
    "    try:\n",
    "        prompt = f\"What does it mean if someone is diagnosed with {diagnosis}? Explain the dementia stage.\"\n",
    "        response = llm_gemini.invoke(prompt)\n",
    "        return response.content if hasattr(response, \"content\") else str(response)\n",
    "    except Exception as e:\n",
    "        return f\"❌ Gemini stage summary error: {str(e)}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Define the summary generator function\n",
    "def generate_gemini_summary(input_text):\n",
    "    try:\n",
    "        prompt = f\"Explain in simple clinical terms: {input_text}\"\n",
    "        response = llm_gemini.invoke(prompt)\n",
    "        return response.content if hasattr(response, \"content\") else str(response)\n",
    "    except Exception as e:\n",
    "        return f\"❌ Gemini error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot response\n",
    "\n",
    "def dementia_chatbot(image, text_input):\n",
    "    if image is None:\n",
    "        return \"Please upload an MRI image.\"\n",
    "\n",
    "    mri_probs = predict_dementia_stage(image)\n",
    "    if text_input:\n",
    "        text_probs = classify_text_to_dementia(text_input)\n",
    "        final_probs = combine_predictions(mri_probs, text_probs)\n",
    "    else:\n",
    "        final_probs = mri_probs\n",
    "\n",
    "    idx = int(np.argmax(final_probs))\n",
    "    confidence = final_probs[idx]\n",
    "    diagnosis = label_map[idx]\n",
    "    prob_text = \"\\n\".join([f\"{label_map[i]}: {p:.2f}\" for i, p in enumerate(final_probs)])\n",
    "    \n",
    "\n",
    "    response = f\"🧠 **Predicted Dementia Stage**: {diagnosis} (Confidence: {confidence:.2f})\\n\"\n",
    "    response += f\"📊 **Combined Probabilities**:\\n{prob_text}\\n\"\n",
    "    if text_input:\n",
    "        response += \"💬 **Entered Text Analysis**: Combined rule-based + BERT sentiment (40% weight).\\n\"\n",
    "    # ✅ Add Gemini MRI interpretation summary\n",
    "    try:\n",
    "        explanation = generate_gemini_summary(f\"MRI indicates: {diagnosis} with {confidence:.2f} confidence\")\n",
    "        response += f\"\\n🧬 **Gemini MRI Interpretation:**\\n{explanation}\\n\"\n",
    "    except Exception as e:\n",
    "        response += f\"\\n❌ Gemini MRI Interpretation Error: {e}\\n\"\n",
    "\n",
    "    # ✅ Add Gemini explanation of the dementia stage\n",
    "    try:\n",
    "        stage_summary = generate_dementia_stage_summary(diagnosis)\n",
    "        response += f\"\\n📘 **Gemini Stage Explanation:**\\n{stage_summary}\\n\"\n",
    "    except Exception as e:\n",
    "        response += f\"\\n❌ Gemini Stage Explanation Error: {e}\\n\"\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://5526421984f64cd7ea.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5526421984f64cd7ea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:32:48.116814: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
     ]
    }
   ],
   "source": [
    "# Gradio app interface\n",
    "demo = gr.Interface(\n",
    "    fn=dementia_chatbot,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"filepath\", label=\"Upload Brain MRI Image\"),\n",
    "        gr.Textbox(lines=2, placeholder=\"Enter symptoms or previous diagnosis (optional)\", label=\"Symptom Text\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"🧠 Dementia MRI Chatbot (MRI + Text Analysis)\",\n",
    "    description=\"Upload a brain MRI and optionally describe symptoms. MRI contributes 60%, and combined rule-based + sentiment text classification contributes 40% to the final result.\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
